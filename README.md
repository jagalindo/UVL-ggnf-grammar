# Llama.UVL

## Project Description
Llama.UVL is an innovative project that focuses on generating UVL serializations of feature models using llama.cpp. A grammar defined in BNF format is utilized and fed to llama.cpp. The project incorporates instruction prompting and allows for various parameter modifications, including context window, number of tokens to predict, and leveraging existing models as templates.

## Installation and Setup
To get started with Llama.UVL, Google Colab is required. It is essential to have an NVIDIA graphics card to avoid additional configurations.

### Steps:
1. Open Google Colab.
2. Navigate to GitHub integration.
3. Open the Llama.UVL project.
4. Execute all the cells in the notebook.

Note: The prompts generation is optional, as the necessary calculations are already included in this repository.

## Usage
To use Llama.UVL, simply follow the installation and setup steps. Once set up, you can interact with the project through the Google Colab interface. Modifications to parameters and prompts can be made according to your requirements.

## Running Tests
Currently, there are no automated tests defined for this project.

## Deployment
There are no special deployment requirements for Llama.UVL. The project is designed to run seamlessly in the Google Colab environment.

## Built With
* Python
* llama.cpp
* Node.js (Note: Node code is pending cleanup)

## Contributing
We welcome contributions! If you're interested in contributing, feel free to fork the repository and make it your own.

## Authors and Acknowledgments
* **PhD José Á Galindo** - [personales.us.es/jagalindo](https://personales.us.es/jagalindo)
* **Antonio Dominguez** - AI Director at Datacrunch

Special thanks to all contributors and supporters of the Llama.UVL project.

## License
This project is licensed under the GPL-3.0 License - see the [LICENSE](LICENSE) file for details.
